#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
@author: Andreas Georgopoulos
"""

from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import statsmodels.api as sm
from tqdm import tqdm
import pandas as pd
import numpy as np
import itertools
import warnings
import pylab
import time
import math
import os


# Set Working Directory (change to yours)
os.chdir('/Users/macuser/Desktop/Research/Organic/organic_data')

# Read files
network_impressions = pd.read_csv('network_impressions_modeling.csv') # generated by preprocessing file


"""
    ###########################################################################
    ######################### Probabilistic Model #############################
    ###########################################################################
"""

# Channels: different site_ids that user was exposed to specific impression (28 channels)
network_impressions.site_ID = network_impressions.site_ID.astype('category')


# Simple Empirical Probability of each channel ------------------------------------------------

# Initialise N_pos, N_neg of each channel
channel_pos_neg = pd.DataFrame({'channel':network_impressions.groupby('site_ID').size().index.astype('int'), 'N_pos': 0, 'N_neg': 0,'Total_Impressions': network_impressions.groupby('site_ID').size().tolist()})
channel_pos_neg.set_index('channel', inplace=True)

# N_pos(channel_i) if immediate conversion or conversion (immediate from another channel) and user exposed to channel_i on same day
# Immediate conversions
for ch in channel_pos_neg.index.tolist():
    channel_pos_neg.loc[ch, 'N_pos'] += len(network_impressions[(network_impressions.site_ID == ch) & (network_impressions.activity_conversion == 1)])

# Not immediate converted impressions
net_impr_date = network_impressions.loc[(network_impressions.activity_conversion == 1),['user_id','date','Date','site_ID']]
for im in range(len(net_impr_date)):
    user = net_impr_date.user_id.iloc[im]
    date = net_impr_date.Date.iloc[im]
    site = net_impr_date.site_ID.iloc[im]
    # Extract all other channels/sites that user was exposed at specific Date of the converted impression
    exp_channels = list(set(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date == date) & (network_impressions.site_ID != site),'site_ID']))
    if(len(exp_channels) > 0):
        # user was exposed to additional sites/channels:
        for j in range(len(exp_channels)):    
        # Add N_pos
            channel_pos_neg.loc[exp_channels[j], 'N_pos'] += 1

# Probability of conversion of each channel:
channel_pos_neg['Prob_conversion'] = round(channel_pos_neg.N_pos / channel_pos_neg.Total_Impressions, 3)
# Save df
channel_pos_neg.to_csv('channel_pos_neg.csv')


# Conditional Probabilities of each pair of channels i,j -----------------------------------------
channels_pairs_pos_neg = pd.DataFrame({'channels_pair':list(itertools.combinations(network_impressions.groupby('site_ID').size().index.astype('int'), 2)), 'N_pos': 0, 'N_neg': 0})
channels_pairs_pos_neg.set_index('channels_pair', inplace=True)

# N_positive
net_impr_date = network_impressions.loc[(network_impressions.activity_conversion == 1),['user_id','date','Date','site_ID']]
for im in range(len(net_impr_date)):
    user = net_impr_date.user_id.iloc[im]
    date = net_impr_date.Date.iloc[im]
    site = net_impr_date.site_ID.iloc[im]
    # Extract all other channels/sites that user was exposed at specific Date of the converted impression
    exp_channels = list(set(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date == date),'site_ID']))
    exp_channels = sorted(exp_channels)
    if(len(exp_channels) > 1):
        # user was exposed to additional sites/channels:
        
        channels_comb_2 = list(itertools.combinations(exp_channels, 2))
        for j in range(len(channels_comb_2)):    
        # Add N_pos
            channels_pairs_pos_neg.loc[[channels_comb_2[j]],'N_pos'] += 1
    
# N_negative
# Extract impressions that were not in any day of a resulted conversion
remove_impressions = network_impressions.loc[(network_impressions.activity_conversion == 1),['user_id','Date']].apply(tuple, axis=1)
network_impressions['tuple_user_date'] = network_impressions[['user_id','Date']].apply(tuple, axis=1)
net_impr_date_neg = network_impressions[~network_impressions['tuple_user_date'].isin(remove_impressions)]
del network_impressions['tuple_user_date']
del net_impr_date_neg['tuple_user_date']

for im in tqdm(range(len(net_impr_date_neg))):
    user = net_impr_date_neg.user_id.iloc[im]
    date = net_impr_date_neg.Date.iloc[im]
    site = net_impr_date_neg.site_ID.iloc[im]
    # Extract all other channels/sites that user was exposed to at a given date where no conversion took place
    exp_channels = list(set(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date == date),'site_ID']))
    exp_channels = sorted(exp_channels)
    if(len(exp_channels) > 1):
        # user was exposed to additional sites/channels:
        channels_comb_2 = list(itertools.combinations(exp_channels, 2))
        for j in range(len(channels_comb_2)):    
        # Add N_pos
            channels_pairs_pos_neg.loc[[channels_comb_2[j]],'N_neg'] += 1

    time.sleep(0.01)


# Conditional Probability of each pair of channels:
channels_pairs_pos_neg['Prob_conversion'] = round(channels_pairs_pos_neg.N_pos / (channels_pairs_pos_neg.N_pos + channels_pairs_pos_neg.N_neg), 3)

# Drop Pairs where no obsis recorded
channels_pairs_pos_neg = channels_pairs_pos_neg[channels_pairs_pos_neg.Prob_conversion >= 0]

# Save df
channels_pairs_pos_neg.to_csv('channels_pairs_pos_neg.csv')



# Contribution of each channel  ---------------------------------------------------
channel_contribution = pd.DataFrame({'channel':network_impressions.groupby('site_ID').size().index.astype('int'), 'Contribution': 0})
channel_contribution.set_index('channel', inplace=True)

for ch in channel_pos_neg.index.tolist():
    # Extract all pairs that this channel exist
    pairs = [item for item in channels_pairs_pos_neg.index.tolist() if ch in item]
    other_channels = [x[1] if x[0] == ch else x[0] for x in pairs]
    if other_channels:
        second_order = (sum(channels_pairs_pos_neg.loc[pairs, 'Prob_conversion']) - len(pairs)*channel_pos_neg.loc[ch,'Prob_conversion'] - sum(channel_pos_neg.loc[other_channels,'Prob_conversion'])) / (2*len(pairs))
    else:
        second_order = 0
    # Contribution: empirical + second_order
    channel_contribution.loc[ch, 'Contribution'] = channel_pos_neg.loc[ch,'Prob_conversion'] + second_order

# Save df
channel_contribution = channel_contribution.sort_values(by = 'Contribution', ascending = False)
channel_contribution.to_csv('channel_contribution.csv')    
# channel_contribution = pd.read_csv('channel_contribution.csv')






"""
    ###########################################################################
    ############################## ML Modelling ###############################
    ###########################################################################
"""

# Transformed Dataset ----------------------------------------------------------------

def time_to_sec(x):
    """
        Input: string format of time in format h:m:s
        Output: seconds
    """
    x = time.strptime(x,'%H:%M:%S')
    return timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()

# For each user and date find channels that he or she was exposed and result of attribution, as well as: total time of exposure, number of ads shown
dataframe_impressions_user_date = network_impressions.drop_duplicates(subset=['user_id','Date'])[['user_id','Date']]
dataframe_impressions_user_date['channels'] = 'na'
dataframe_impressions_user_date['activity_conversion'] = 'na'
dataframe_impressions_user_date['number_of_displayed_ads'] = 'na'
dataframe_impressions_user_date['new_user'] = 'na'
dataframe_impressions_user_date['converted_user'] = 'na'
dataframe_impressions_user_date['time_following_user_sec'] = 'na'                               
                               
for c in tqdm(dataframe_impressions_user_date.index.tolist()):
    user = dataframe_impressions_user_date.user_id.loc[c]
    date = dataframe_impressions_user_date.Date.loc[c]
    # Extract data of specific user and date                              
    impre_user_date = network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date == date),['site_ID','Time','activity_conversion']]

    # If there is a conversion then take channels before this conversion
    if (len(impre_user_date.loc[(impre_user_date.activity_conversion == 1),'Time']) != 0 ):
        # Channels ---------------------------------------------------
        dataframe_impressions_user_date.set_value(c, 'channels', list(set(impre_user_date.loc[impre_user_date.Time <= (impre_user_date.loc[(impre_user_date.activity_conversion == 1),'Time'].values[0]), 'site_ID'])))
        #dataframe_impressions_user_date.loc[c, 'channels'] = list(set(impre_user_date.loc[impre_user_date.Time <= (impre_user_date.loc[(impre_user_date.activity_conversion == 1),'Time']), 'site_ID']))
        dataframe_impressions_user_date.loc[c, 'activity_conversion'] = 1
        # Number of ads  ------------------------------ --------------  
        dataframe_impressions_user_date.loc[c, 'number_of_displayed_ads'] = len(impre_user_date.loc[impre_user_date.Time <= impre_user_date.loc[(impre_user_date.activity_conversion == 1),'Time'].values[0], 'site_ID'])
        # New user (has any impression in the past) ------------------
        dataframe_impressions_user_date.loc[c, 'new_user'] = 1 if len(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date < date), 'activity_conversion']) == 0 else 0   
        # Returned user (has any conversion in the past) -------------
        dataframe_impressions_user_date.loc[c, 'converted_user'] = 0 if sum(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date < date), 'activity_conversion']) == 0 else 1   
    else:
        # take all channels
        dataframe_impressions_user_date.set_value(c, 'channels', list(set(impre_user_date.site_ID)))
        dataframe_impressions_user_date.loc[c, 'activity_conversion'] = 0
        # Number of ads  ------------------------------ --------------  
        dataframe_impressions_user_date.loc[c, 'number_of_displayed_ads'] = len(impre_user_date)
        # New user (has any impression in the past) ------------------
        dataframe_impressions_user_date.loc[c, 'new_user'] = 1 if len(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date < date), 'activity_conversion']) == 0 else 0
        # Returned user (has any conversion in the past) -------------
        dataframe_impressions_user_date.loc[c, 'converted_user'] = 0 if sum(network_impressions.loc[(network_impressions.user_id == user) & (network_impressions.Date < date), 'activity_conversion']) == 0 else 1

    # Total time 'following' user (seconds) [need to find unique time intervals during the day, count seconds and sum all of them at the end]
    hours = list(set(impre_user_date.Time.str.split(':', 1, expand = True)[0]))
    total_time = 0
    for h in hours:
        min_time = min(impre_user_date.loc[impre_user_date.Time.str.startswith(h),'Time'])
        max_time = max(impre_user_date.loc[impre_user_date.Time.str.startswith(h),'Time'])
        
        total_time += (time_to_sec(max_time) - time_to_sec(min_time))       
    dataframe_impressions_user_date.loc[c, 'time_following_user_sec'] = total_time

    time.sleep(0.01)

# Save df
dataframe_impressions_user_date.to_csv('dataframe_impressions_user_date.csv', index = False)
# Read df
#dataframe_impressions_user_date = pd.read_csv('dataframe_impressions_user_date.csv')
# dataframe_impressions_user_date.channels = [eval(x) for x in dataframe_impressions_user_date.channels]


# Create a set of dummy variables for each channel label 
channel_dummies = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.channels.str.len() > 0, 'channels'].apply(lambda x: pd.Series([1] * len(x), index=x)).fillna(0, downcast='infer')
# Join with initial dataset
dataframe_impressions_user_date = dataframe_impressions_user_date.join(channel_dummies)
# Remove channels column
dataframe_impressions_user_date.drop('channels', axis = 1, inplace = True)
# Categorical target variable
dataframe_impressions_user_date.activity_conversion = dataframe_impressions_user_date.activity_conversion.astype('category')

# Save df
dataframe_impressions_user_date.to_csv('dataframe_impressions_user_date_extended.csv', index = False)
#dataframe_impressions_user_date = pd.read_csv('dataframe_impressions_user_date_extended.csv')


"""
# Dataframe plot (bining)
dataframe_impressions_user_date_plot = dataframe_impressions_user_date

bins_time = [-1,60,300,900,1800,3600,7200,21308]
bins_names_time = ['[0,60sec]','(60,300sec]','(300,900sec]','(900,1800sec]','(1800,3600sec]','(3600,7200sec]','(7200-21307sec]']

dataframe_impressions_user_date_plot['Bins_Time'] = pd.cut(dataframe_impressions_user_date_plot.time_following_user_sec, bins_time, labels=bins_names_time)

# Save df to plot
dataframe_impressions_user_date_plot.to_csv('dataframe_impressions_user_date_plot.csv',index = False)
"""

# Training - Test Sets --------------------------------------------------------

# Randomly split unique users in training and test sets
users = list(set(dataframe_impressions_user_date.user_id))
np.random.shuffle(users)
training_users, test_users = users[:math.ceil(0.8*len(users))], users[math.ceil(0.8*len(users)):]

# Split target and regressors on train-test sets based on users split
training_target = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(training_users), 'activity_conversion']
test_target = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(test_users), 'activity_conversion']

training_regressors = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(training_users), dataframe_impressions_user_date.columns.difference(['activity_conversion','user_id','Date']).tolist()]
test_regressors = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(test_users), dataframe_impressions_user_date.columns.difference(['activity_conversion','user_id','Date']).tolist()]



"""
    Logistic Regression -------------------------------------------------------
"""

lr_clf =  LogisticRegression()
lr_clf_model = lr_clf.fit(training_regressors, training_target)
lr_clf_model.coef_
lr_clf_model.intercept_

predictions = lr_clf_model.predict(test_regressors)

accuracy_score(test_target, predictions)
print(confusion_matrix(test_target, predictions))



""" 
    Random Forest  ------------------------------------------------------------
"""

model = RandomForestClassifier(random_state = 1991)
# Tune FR hyper parameters 'n_estimators' & 'min_samples_leaf'
param_grid = { 
    #'n_estimators': [1, 5, 10, 20, 50, 60, 70, 80, 90, 100, 150, 200],
    'n_estimators': [100, 200, 400],
    'min_samples_leaf': [1, 2, 5]
}
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5, verbose = 2) 
grid.fit(training_regressors, training_target) 
# Optimal Hyper-parameters based on GridSearch for RF
optimal_trees = grid.best_estimator_.n_estimators    
optimal_leaf = grid.best_estimator_.min_samples_leaf 

# RFC with best parameters
clf = RandomForestClassifier(n_estimators = 100, min_samples_leaf = 2, random_state = 1991)
clf.fit(training_regressors, training_target)           

predictions_rfc = clf.predict(test_regressors)

accuracy_score(test_target, predictions_rfc)
print(confusion_matrix(test_target, predictions_rfc))



# SMOTE Oversampling

sm = SMOTE(random_state=1991, ratio = "auto")
regressors_train, target_train = sm.fit_sample(training_regressors, training_target)

# RFC with best parameters
clf = RandomForestClassifier(n_estimators = 400, min_samples_leaf = 2, random_state = 1991)
clf.fit(regressors_train, target_train)           

predictions_rfc = clf.predict(test_regressors)

accuracy_score(test_target, predictions_rfc)
print(confusion_matrix(test_target, predictions_rfc))



## Random Forest
def random_forest_func(pred_train, pred_test, tar_train, tar_test):
    #Build model on training data  
    classifier=RandomForestClassifier(n_estimators = 400, min_samples_leaf = 2, random_state = 1991)
    classifier=classifier.fit(pred_train,tar_train)
    
    # fit an Extra Trees model to the data
    model = ExtraTreesClassifier()
    model.fit(pred_train,tar_train)
    
    #getting the training and test accuracies from random forest classifier
    #train_accuracy
    predictions = classifier.predict(pred_train)
    train_accuracy = accuracy_score(tar_train, predictions)
    #test_accuracy
    predictions = classifier.predict(pred_test)
    test_accuracy = accuracy_score(tar_test, predictions)
    
    # display the relative importance of each attribute from extra trees classifier
    feature_importance = model.feature_importances_

    return feature_importance, train_accuracy, test_accuracy


# Random Forest Importance of features
importance,train_accuracy,test_accuracy = random_forest_func(regressors_train, test_regressors, target_train, test_target) 

predictors_importance = pd.DataFrame(np.matrix([importance.tolist(),training_regressors.columns.tolist()]).T)
predictors_importance[0] = round(predictors_importance[0].astype(float), 6)
predictors_importance = predictors_importance.sort_values(by = 0, ascending = False)

# Barplot of importance
def plot_importance(rank, regressors,chart_title):
    # Sort predictors by importance level
    var_imp = rank.tolist()
    sorted_predictors = pd.DataFrame(np.matrix([var_imp,regressors]).T)
    sorted_predictors[0] = sorted_predictors[0].astype(float)
    sorted_predictors = sorted_predictors.sort_values(by = 0, ascending = False)

    # Barplot of importance
    fig, ax = plt.subplots(figsize=(10,10))

    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)

    pos = pylab.arange(len(sorted_predictors))+.5 
    plt.barh(pos,sorted_predictors[0], align='center')
    plt.yticks(pos, sorted_predictors[1], fontsize=12)
    plt.xticks(fontsize=12)
    plt.xlabel('Variable Importance', fontsize=13)
    plt.title(chart_title, fontsize=16, weight="bold")
    plt.savefig(chart_title, bbox_inches='tight')
    plt.show()

plot_importance(importance, training_regressors.columns.tolist(), "Random Forest - Predictors' Relative Importance")




"""
    Bagged Logit Model --------------------------------------------------------
"""
warnings.filterwarnings('ignore')

users_conv = list(set(dataframe_impressions_user_date.loc[dataframe_impressions_user_date.activity_conversion == 1, 'user_id']))
user_non_conv = list(set(dataframe_impressions_user_date.loc[dataframe_impressions_user_date.activity_conversion == 0, 'user_id']))

# Do bagging (M=1000) and save coefficient estimates
coef_estimates = pd.DataFrame(columns = training_regressors.columns.tolist())
coef_intercept = pd.DataFrame(columns = ['Intercept'])
# Pvalues
p_values = pd.DataFrame(None, index=np.arange(10000), columns=training_regressors.columns.tolist())
p_values['const'] = None

n = 0
while n < 10000:
    # Shuffle lists of users and extract 140 converted - 700 non converted users for training (ratio 1:5) and 34 conv - 170 nonconv for test (ratio 1:5)
    np.random.shuffle(users_conv)
    np.random.shuffle(user_non_conv)

    training_users = users_conv[:math.ceil(len(users_conv) * 0.8)] + user_non_conv[:math.ceil(len(users_conv) * 0.8)*5]
    test_users = users_conv[math.ceil(len(users_conv) * 0.8):] + user_non_conv[(math.ceil(len(users_conv) * 0.8)*5):((math.ceil(len(users_conv) * 0.8)*5) + (math.ceil(len(users_conv) * 0.2)-1)*5)]                  


    # Split target and regressors on train-test sets based on users split
    training_target = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(training_users), 'activity_conversion']
    test_target = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(test_users), 'activity_conversion']

    training_regressors = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(training_users), dataframe_impressions_user_date.columns.difference(['activity_conversion','user_id','Date']).tolist()]
    test_regressors = dataframe_impressions_user_date.loc[dataframe_impressions_user_date.user_id.isin(test_users), dataframe_impressions_user_date.columns.difference(['activity_conversion','user_id','Date']).tolist()]

    lr_clf =  LogisticRegression()
    lr_clf.fit(training_regressors, training_target)
    coef_estimates = coef_estimates.append(pd.DataFrame(lr_clf.coef_, columns=training_regressors.columns.tolist()),ignore_index=True)
    coef_intercept = coef_intercept.append(pd.DataFrame(lr_clf.intercept_, columns=['Intercept']),ignore_index=True)


    # P-values
    
#    try:
        # Remove columns with no variation (will bag logit from statsmodel if included)
#        drop_cols = training_regressors.sum(axis = 0)[training_regressors.sum(axis = 0) == 0].index.tolist()
        # Drop Columns with no variation
#        regressors_train_logit = training_regressors.drop(drop_cols, axis = 1)
#        regressors_train_logit = regressors_train_logit.T.drop_duplicates().T
        # Add Intercept
#        regressors_train_logit = sm.add_constant(regressors_train_logit)
        # Logit Regression
#        res = sm.Logit(training_target, regressors_train_logit).fit()
        # Extract p values and save to df
#        for var in res.pvalues.index.tolist():
#            p_values[var].iloc[n] = res.pvalues[var]
#    except:
#        pass
    
    n += 1
    
    if(n%500 == 0):
        print(n)


warnings.resetwarnings()

# Average Coefficient Estimation of each channel
coef_estimates_mean = pd.DataFrame(coef_estimates.mean())
coef_estimates_mean.columns = ['CoefficientEstimation']

coef_estimates_mean = coef_estimates_mean.sort_values(by = 'CoefficientEstimation', ascending = False)

# Add Intercept Avg Coefficient
coef_estimates_mean.loc['intercept'] = coef_intercept.mean().tolist()[0]
coef_estimates_mean.to_csv('coef_estimates_mean.csv')

# Pvalues
p_values = p_values.dropna(how = 'all')
p_values_mean = pd.DataFrame(round(p_values.mean(),6))
p_values_mean.columns = ['Average_Pvalue']

p_values_mean = p_values_mean.sort_values(by = 'Average_Pvalue', ascending = True)
p_values_mean.to_csv('p_values_mean.csv')    


"""
    Marginal Probability Effects of each channel of BLM ------------------------
"""
pae_channels = pd.DataFrame({'Channel': dataframe_impressions_user_date.columns.difference(['number_of_displayed_ads','converted_user','new_user','time_following_user_sec','activity_conversion','user_id','Date']).tolist(), 'CoefficientEstimate':0, 'PAE':0})

# Match corresponding coefficient estimation
i = 0
for i in range(len(pae_channels)):
    pae_channels.CoefficientEstimate.iloc[i] = coef_estimates_mean.loc[pae_channels.Channel.iloc[i], 'CoefficientEstimation']
                
# Compute Marginal Effect of each channel with rest binary variables equal to 0 and other features with sample mean    

constant = coef_intercept.Intercept.mean() + coef_estimates_mean.loc['number_of_displayed_ads', 'CoefficientEstimation'] * dataframe_impressions_user_date.number_of_displayed_ads.mean() + coef_estimates_mean.loc['time_following_user_sec','CoefficientEstimation'] * dataframe_impressions_user_date.time_following_user_sec.mean()
for c in pae_channels.Channel.tolist():
    pae_channels.loc[pae_channels.Channel == c, 'PAE'] = (np.exp(constant + coef_estimates_mean.loc[c, 'CoefficientEstimation']) / (1 + np.exp(constant + coef_estimates_mean.loc[c, 'CoefficientEstimation']))) - (np.exp(constant) / (1 + np.exp(constant)))


pae_channels = pae_channels.sort_values(by = 'PAE', ascending = False)

# Save df
pae_channels.to_csv('marginal_effects_channels.csv')    
#pae_channels = pd.read_csv('marginal_effects_channels.csv')    







"""
    ###########################################################################
    ######################## User-Level Attribution ###########################
    ###########################################################################
"""
attr_channels = pd.DataFrame({'Channel': dataframe_impressions_user_date.columns.difference(['number_of_displayed_ads','converted_user','new_user','time_following_user_sec','activity_conversion','user_id','Date']).astype(int).tolist()})


# Last - touch Attribution ------------------------

network_impressions['Time'] = pd.to_datetime(network_impressions.date, format = "%d%b%Y:%H:%M:%S").dt.time
# For each recorded conversion per user and date get last ad  user was exposed to and assign credits only to this one
lta_attr = network_impressions[network_impressions.activity_conversion == 1][['user_id','Date','Time','site_ID']][network_impressions[network_impressions.activity_conversion == 1]\
                               [['user_id','Date','Time','site_ID']]['Time'] == network_impressions[network_impressions.activity_conversion == 1]\
                               [['user_id','Date','Time','site_ID']].groupby(['user_id','Date'])['Time'].transform(max)].groupby('site_ID').size().reset_index(name = 'LTA')
lta_attr.columns = ['Channel','LTA']



# Bagged Logit Model Attribution ------------------

blm_attr = dataframe_impressions_user_date[dataframe_impressions_user_date.activity_conversion == 1][list(map(str, pae_channels[pae_channels.PAE >0]['Channel'].tolist()))].sum(axis = 0).reset_index(name = 'BLM')
blm_attr.columns = ['Channel','BLM']
blm_attr.Channel = blm_attr.Channel.astype(int)


# Probabilistic Model Attribution -----------------

pm_attr = dataframe_impressions_user_date[dataframe_impressions_user_date.activity_conversion == 1][list(map(str, channel_contribution[channel_contribution.Contribution >0]['channel'].tolist()))].sum(axis = 0).reset_index(name = 'PM')
pm_attr.columns = ['Channel','PM']
pm_attr.Channel = pm_attr.Channel.astype(int)


# Join Results ------------------------------------

attr_channels = attr_channels.merge(lta_attr, how = 'left', on = 'Channel')
attr_channels = attr_channels.merge(blm_attr, how = 'left', on = 'Channel')
attr_channels = attr_channels.merge(pm_attr, how = 'left', on = 'Channel')
attr_channels.fillna(0, inplace = True)

attr_channels.to_csv('attr_channels.csv', index = False)